---
title: "Paper"
author: "Mingyang Xue"
date: "6/5/2020"
output: pdf_document
---
```{r, echo=FALSE}
qqnorm_bands <- function(x, method = c("ks", "ts", "both"), mu, sigma, estimate = c("mle","robust"), 
                         M = 5000, alpha = 0.05, title="") {
  
  method <- match.arg(method)
  estimate <- match.arg(estimate)
  
  out <- list()
  bands <- c()
  n <- length(x)
  x <- sort(x)
  if (missing(mu) || missing(sigma)) {
    # unknown parameters
    if (estimate == "mle") {
      mu <- mean(x)
      sigma <- sd(x)
    }
    if (estimate == "robust") {
      library(robustbase) # for robust estimates for the mean and sd
      Qn.scale<-function(x){
        Qn(x,finite.corr=FALSE)
      }
      Qn.location<-function(x){
        s_Qn(x,mu.too=TRUE)[[1]]
      }
      mu <- Qn.location(x)
      sigma <- Qn.scale(x)
    }
  }
  # standardize sample
  x <- (x-mu)/sigma
  p <- (1:length(x) - 0.5)/length(x)
  # quantile based on ruler method
  q <- qnorm(p)
  plot(q, x, xlim = c(-3,3), ylim = c(-3,3), xlab = "Standard Normal Quantiles",
       ylab = "Standardized Sample Quantiles", main = title, pch = 20)
  
  # KS bands
  if (method == "ks" || method == "both") {
    k <- 0.895/(sqrt(n) * (1 - 0.01/sqrt(n) + 0.85/n))
    l <- suppressWarnings(qnorm(p - k))
    u <- suppressWarnings(qnorm(p + k))
    lines(q, l, lty = 2, col = "red")
    lines(q, u, lty = 2, col = "red")
    bands <- cbind("KS.lower.band"=l, "KS.upper.band"=u)
  } 
  
  # TS bands
  if (method == "ts" || method == "both") {
    C <- rep(0,M)
    for (m in 1:M) {
      if (!missing(mu) && !missing(sigma)) {
        # known parameters
        y <- runif(n = n, min = 0, max = 1)
      } else {
        # unknown parameters
        xm <- rnorm(n = n, mean = 0, sd = 1)
        if (estimate == "mle") {
          mu <- mean(xm)
          sigma <- sd(xm)
        }
        if (estimate == "robust") {
          mu <- Qn.location(xm)
          sigma <- Qn.scale(xm)
        }
        zm <- (xm-mu)/sigma
        y <- pnorm(zm)
      }
      y <- sort(y)
      # Find minimum confidence level C_m for each simulation
      p.value <- rep(0, n)
      for (i in 1:n) {
        p.value[i] <- min(c(pbeta(q = y[i], shape1 = i, shape2 = n-i+1), 1 - pbeta(q = y[i], shape1 = i, shape2 = n-i+1)))  
      }
      C[m] <- 2 * min(p.value)
    }
    # Set gamma to be the 100*alpha percentile of C_1,...C_M
    gamma <- quantile(C, probs = alpha)
    l <- rep(0, n)
    u <- rep(0, n)
    for (i in 1:n) {
      l[i] <- qnorm(qbeta(p = gamma/2, shape1 = i, shape2 = n-i+1))
      u[i] <- qnorm(qbeta(p = 1-gamma/2, shape1 = i, shape2 = n-i+1))
    }
    lines(q, l, lty = 2, col = "blue")
    lines(q, u, lty = 2, col = "blue")
    bands <- cbind(bands, "TS.lower.band"=l, "TS.upper.band"=u)
  }
  if (method == "both") {
    legend("topleft", legend = c("ks", "ts"), col = c("red", "blue"), lty = c(2,2), cex = 0.8)
  }
  
  bands <- data.frame(bands)
  out[["bands"]] <- bands
  
  is.reject <- c()
  if (method == "ks" || method == "both") {
    ks.reject <- na.omit(x[(bands$KS.lower.band > x) | (x > bands$KS.upper.band)])
    ks.reject.index <- na.omit(which((bands$KS.lower.band > x) | (x > bands$KS.upper.band)))
    is.reject <- c(is.reject, length(ks.reject) != 0)
    out[["ks.reject"]] <- ks.reject
    out[["ks.reject/index"]] <- ks.reject.index
  }
  
  if (method == "ts" || method == "both") {
    ts.reject <- x[(bands$TS.lower.band > x) | (x > bands$TS.upper.band)]
    ts.reject.index <- which((bands$TS.lower.band > x) | (x > bands$TS.upper.band))
    is.reject <- c(is.reject, length(ts.reject) != 0)
    out[["ts.reject"]] <- ts.reject
    out[["ts.reject/index"]] <- ts.reject.index
  }
  out[["is.reject"]] <- is.reject
  return(out)
}
```

#Examples
According to the description of KS Test, we know KS Test is more sensitive at the center than at the tails, while TS test is more sensitive at the tails than at the center. We also know that TS test is much more sensitive at the tails than KS Test is. But it's unclear to us whether KS Test is more sensitive at the center than TS Test is without the help of graphs. In the following, we generate 100 data that 70 of them come from a standard normal distribution stimulation $N(0,1)$and 30 of them come from exponential distribution stimulation $exp(1)$, and then plot this group of data using the function we conducted in previous section with MLE estimator. In the graph, we can see some data points are outside of KS Confidence Bands, while all data points locate inside the TS Confidence Bands. Since this group of data does not come from normal distribution entirely, we can see TS Test makes incorrect decision in this case, and KS Test performes better.
```{r, echo=FALSE, results='hide', message=FALSE}
x1 <- rnorm(70)
x2 <- rexp(30)
x <- c(x1,x2)
qqnorm_bands(x = x, estimate = "mle", method = "both", title = "Sample rejected by KS Test but accepted by TS Test")
```
On the other hand, TS Test can performe better in some cases. In the following, we generate a group of data from a standard normal distribution stimulation $N(0,1)$ with size $n=200$. Then we sort this group of data, subtract 0.5 from 10 smallest data and add 1 to 10 biggest data. We again plot this group of data using the function we conducted in previous section with MLE estimator. It is obvious that the data we intentionally edit goes beyond the TS Test Confidence Bands but locates inside of the KS Test Confidence Bands.

```{r, echo=FALSE, results='hide', message=FALSE}
x <- rnorm(200)
x <- sort(x)
x1 <- x[1:10] - 0.5
x2 <- x[191:200] + 1
x <- c(x1, x[11:190], x2)
qqnorm_bands(x = x, estimate = "mle", method = "both", title = "Sample rejected by TS Test but accepted by KS Test")
```

#Power Analysis
In this section, we are going to show the power of TS Test compared with KS Test. We use four alternative distributions as listed in the table, and get the power by calculating how many times these two tests reject alternative distributions in 100 repititions of stimulation.
##Known parameter
In this part, we use theoretical means and standard deviations as parameters to plug in the function we constructed in previous sections. The power of these two tests is shown in the table below. From the table, we can see that TS Test has lower power than KS Test except for $\chi^2(1)$, but the difference between these two tests is not huge for $\chi^2(5)$ and $T(4)$. For $Poisson(\lambda=10)$, KS Test has much higher power since most data points we generate locate around the center where TS Test is not sensitive.
\begin{table}[]
\begin{tabular}{|l|l|l|}
\hline
Alternative distributions & KS Test & TS Test \\ \hline
$\chi^2(1)$               & 1       & 1       \\ \hline
$\chi^2(5)$               & 0.58    & 0.36    \\ \hline
$T(4)$                     & 0.5     & 0.34    \\ \hline
$Poisson(\lambda=10)$      & 0.71    & 0.12    \\ \hline
\end{tabular}
\end{table}

```{r, echo=FALSE, fig.show='hide', eval=FALSE}
pw_1.ks <- 0
pw_2.ks <- 0
pw_3.ks <- 0
pw_4.ks <- 0
pw_1.ts <- 0
pw_2.ts <- 0
pw_3.ts <- 0
pw_4.ts <- 0
for (i in 1:100){
  x <- rchisq(50, 1)
  out <- qqnorm_bands(x = x, mu = 1, sigma = sqrt(2), method = "both", M = 1000)
  if (out$is.reject[1] == TRUE){
    pw_1.ks <- pw_1.ks + 0.01
  }
  if (out$is.reject[2] == TRUE){
    pw_1.ts <- pw_1.ts + 0.01
  }
  x <- rchisq(50, 5)
  out <- qqnorm_bands(x = x, mu = 5, sigma = sqrt(10), method = "both", M = 1000)
  if (out$is.reject[1] == TRUE){
    pw_2.ks <- pw_2.ks + 0.01
  }
  if (out$is.reject[2] == TRUE){
    pw_2.ts <- pw_2.ts + 0.01
  }
  x <- rt(50, 4)
  out <- qqnorm_bands(x = x, mu = 0, sigma = sqrt(2), method = "both", M = 1000)
  if (out$is.reject[1] == TRUE){
    pw_3.ks <- pw_3.ks + 0.01
  }
  if (out$is.reject[2] == TRUE){
    pw_3.ts <- pw_3.ts + 0.01
  }
  x <- rpois(50, 10)
  out <- qqnorm_bands(x = x, mu = 10, sigma = sqrt(10), method = "both", M = 1000)
  if (out$is.reject[1] == TRUE){
    pw_4.ks <- pw_4.ks + 0.01
  }
  if (out$is.reject[2] == TRUE){
    pw_4.ts <- pw_4.ts + 0.01
  }
}
pw_1.ks
pw_1.ts
pw_2.ks
pw_2.ts
pw_3.ks
pw_3.ts
pw_4.ks
pw_4.ts
```

##Unknown Parameters
In this section, we use the same four alternative distributions but calculate the power in the case that we don't know parameters. For two chi-square distributions and one possion distribution, we use mle as estimator, and for student t distribution, we use robust estimator since mle works poorly for this distribution. The power we calculated is shown in the table below. From the table, we can see TS Test has higher power only in $T(4)$, the reason is that this distribution has heavy tail compared to others.
\begin{table}[]
\begin{tabular}{|l|l|l|}
\hline
Alternative distributions & KS Test & TS Test \\ \hline
$\chi^2(1)$               & 1       & 1       \\ \hline
$\chi^2(5)$               & 0.45    & 0.24    \\ \hline
$T(4)$                    & 0.06    & 0.25    \\ \hline
$Poisson(\lambda=10)$     & 0.25    & 0       \\ \hline
\end{tabular}
\end{table}

```{r, echo=FALSE, fig.show='hide', eval=FALSE}
pw_1.ks <- 0
pw_2.ks <- 0
pw_3.ks <- 0
pw_4.ks <- 0
pw_1.ts <- 0
pw_2.ts <- 0
pw_3.ts <- 0
pw_4.ts <- 0
for (i in 1:100){
  x <- rchisq(50, 1)
  out <- qqnorm_bands(x = x, estimate = "mle", method = "both", M = 1000)
  if (out$is.reject[1] == TRUE){
    pw_1.ks <- pw_1.ks + 0.01
  }
  if (out$is.reject[2] == TRUE){
    pw_1.ts <- pw_1.ts + 0.01
  }
  x <- rchisq(50, 5)
  out <- qqnorm_bands(x = x, estimate = "mle", method = "both", M = 1000)
  if (out$is.reject[1] == TRUE){
    pw_2.ks <- pw_2.ks + 0.01
  }
  if (out$is.reject[2] == TRUE){
    pw_2.ts <- pw_2.ts + 0.01
  }
  x <- rt(50, 4)
  out <- qqnorm_bands(x = x, estimate = "robust", method = "both", M = 1000)
  if (out$is.reject[1] == TRUE){
    pw_3.ks <- pw_3.ks + 0.01
  }
  if (out$is.reject[2] == TRUE){
    pw_3.ts <- pw_3.ts + 0.01
  }
  x <- rpois(50, 10)
  out <- qqnorm_bands(x = x, estimate = "mle", method = "both", M = 1000)
  if (out$is.reject[1] == TRUE){
    pw_4.ks <- pw_4.ks + 0.01
  }
  if (out$is.reject[2] == TRUE){
    pw_4.ts <- pw_4.ts + 0.01
  }
}
pw_1.ks
pw_1.ts
pw_2.ks
pw_2.ts
pw_3.ks
pw_3.ts
pw_4.ks
pw_4.ts
```